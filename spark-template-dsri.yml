apiVersion: template.openshift.io/v1
kind: Template
labels:
  template: spark
message: |-
  The following components have been scheduled for creation in your project: Spark master, Spark master UI, Jupyter
  Go to project overview -> once the creation of all the components are finished, use the following links -
  To access Jupyter: ${CLUSTER_NAME}-jupyter.${APPLICATION_DOMAIN_SUFFIX}
  To access Spark UI: ${CLUSTER_NAME}-spark.${APPLICATION_DOMAIN_SUFFIX}

metadata:
  annotations:
    description: Deploys Apache Spark cluster with Jupyter Lab.
      For more information regarding the usage of this setup (including information of different variables), see - https://maastrichtu-ids.github.io/dsri-documentation/docs/deploy-spark
      The JupyterLab container run with root permissions, see https://github.com/vemonet/jupyterlab-spark


      By default, the setup deploys a spark cluster with 4 Workers, 1 Master and 1 Jupyter pod. Spark code ran via Jupyter notebook/lab automatically runs on the cluster.
      The configuration for the worker, master and jupyter pods can be changed according to the Openshift Quota(Limit Range).
      To get more quota, you should contact Openshift admins.


      WARNING- This deployment setup is still in the experimental stage.
    iconClass: icon-hadoop
    openshift.io/display-name: Apache Spark (Persistent)
    openshift.io/documentation-url: https://github.com/vemonet/spark-openshift
    openshift.io/support-url: https://maastrichtu-ids.github.io/dsri-documentation/help
    openshift.io/long-description: Apache Spark is a popular framework for processing 
      large amounts of data, see - https://spark.apache.org
      Jupyter is a web based application which allows you to write python code in a browser, see - https://jupyter.org
      To learn more about Spark configurations, see - https://spark.apache.org/docs/latest/configuration.html
      To learn Spark programming, see - https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html
    openshift.io/provider-display-name: Institute of Data Science, UM
    tags: spark, jupyter, python
    template.openshift.io/bindable: "false"
  name: apache-spark

objects:
- apiVersion: "v1"
  kind: "PersistentVolumeClaim"
  metadata:
    name: ${CLUSTER_NAME}-pvc
  spec:
    accessModes:
      - "ReadWriteMany"
    resources:
      requests:
        storage: ${STORAGE_SIZE}

- apiVersion: v1
  kind: Secret
  metadata:
    name: ${CLUSTER_NAME}-secret
    labels:
      app: "${CLUSTER_NAME}"
  type: Opaque
  stringData:
    secret.env: |-
      USER=${USERNAME}
      PASS=${PASSWORD}

- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: ${CLUSTER_NAME}-nginx-config-spark
    labels:
      app: "${CLUSTER_NAME}"
  data:
    default.conf: |
      upstream node {
        server localhost:8080;
      }

      server {
          server_name             _;
          listen                  9001;

          location / {
              proxy_set_header X-Real-IP \$remote_addr;
              proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
              proxy_set_header Host ${CLUSTER_NAME}-spark.${APPLICATION_DOMAIN_SUFFIX};
              proxy_pass http://node;
              proxy_redirect off;
              port_in_redirect off;
              auth_basic "Spark UI Login";
              auth_basic_user_file /etc/nginx/secrets/passwd;
          }
      }

# - apiVersion: v1
#   kind: ConfigMap
#   metadata:
#     name: ${CLUSTER_NAME}-nginx-config-jupyter
#   data:
#     default.conf: |-
#       upstream node {
#         server localhost:8888;
#       }
#       client_max_body_size 0;
#       server {
#           server_name             _;
#           listen                  8080;
#           location / {
#               proxy_set_header X-Real-IP \$remote_addr;
#               proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
#               proxy_set_header Host ${CLUSTER_NAME}-jupyter.${APPLICATION_DOMAIN_SUFFIX};
#               proxy_http_version    1.1;
#               proxy_set_header Upgrade $http_upgrade;
#               proxy_set_header Connection "upgrade";
#               proxy_read_timeout    86400;
#               proxy_pass http://node;
#               proxy_redirect off;
#               port_in_redirect off;
#               auth_basic "Jupyter Login";
#               auth_basic_user_file /etc/nginx/secrets/passwd;
#           }
#       }

# - apiVersion: v1
#   kind: ConfigMap
#   metadata:
#     name: ${CLUSTER_NAME}-jupyter-notebook-config
#     labels:
#       app: "${CLUSTER_NAME}"
#   data:
#     notebook_config.py: |-
#       # Configuration file for ipython-notebook.

#       c = get_config()
#       # ------------------------------------------------------------------------------
#       # NotebookApp configuration
#       # ------------------------------------------------------------------------------

#       c.IPKernelApp.pylab = 'inline'
#       c.NotebookApp.ip = '127.0.0.1'
#       c.NotebookApp.open_browser = False
#       c.NotebookApp.port = 8888
#       c.NotebookApp.base_url = '/'
#       c.NotebookApp.trust_xheaders = True
#       c.NotebookApp.tornado_settings = {'static_url_prefix': '/static/'}
#       c.NotebookApp.notebook_dir = '/mnt/${CLUSTER_NAME}-pvc'
#       c.NotebookApp.allow_origin = '*'
#       c.NotebookApp.token = ''
#       c.NotebookApp.password = ''
#       c.NotebookApp.allow_remote_access = True

- apiVersion: v1
  kind: Service
  metadata:
    name: ${CLUSTER_NAME}-spark-master
    labels:
      app: "${CLUSTER_NAME}"
  spec:
    ports:
      - port: 7077
        targetPort: 7077
    selector:
      app: ${CLUSTER_NAME}-spark-master

- apiVersion: v1
  kind: Service
  metadata:
    name: ${CLUSTER_NAME}-spark-master-ui
    labels:
      app: "${CLUSTER_NAME}"
  spec:
    ports:
      - port: 80
        targetPort: 9001
    selector:
      app: ${CLUSTER_NAME}-spark-master

- apiVersion: v1
  kind: Service
  metadata:
    name: ${CLUSTER_NAME}-jupyter-notebook
    labels:
      app: "${CLUSTER_NAME}"
  spec:
    ports:
    - port: 80
      targetPort: 8888
      protocol: TCP
      name: ${CLUSTER_NAME}-jupyter-notebook-service
    selector:
      app: ${CLUSTER_NAME}-jupyter-notebook

- apiVersion: v1
  kind: Route
  metadata:
    name: ${CLUSTER_NAME}-spark-master-route
    labels:
      app: "${CLUSTER_NAME}"
  spec:
    host: ${CLUSTER_NAME}-spark.${APPLICATION_DOMAIN_SUFFIX}
    path: /
    to:
      kind: Service
      name: ${CLUSTER_NAME}-spark-master-ui
    tls:
      insecureEdgeTerminationPolicy: Redirect
      termination: edge

- apiVersion: v1
  kind: Route
  metadata:
    name: ${CLUSTER_NAME}-jupyter-notebook-route
    labels:
      app: "${CLUSTER_NAME}"
  spec:
    host: ${CLUSTER_NAME}-jupyter.${APPLICATION_DOMAIN_SUFFIX}
    path: /
    to:
      kind: Service
      name: ${CLUSTER_NAME}-jupyter-notebook
    tls:
      insecureEdgeTerminationPolicy: Redirect
      termination: edge


- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    annotations:
      description: Spark master
      template.alpha.openshift.io/wait-for-ready: "true"
    name: ${CLUSTER_NAME}-spark-master
    labels:
      app: "${CLUSTER_NAME}"
  spec:
    replicas: 1
    template:
      metadata:
        labels:
          app: ${CLUSTER_NAME}-spark-master
      spec:
        nodeSelector:
          dsri.unimaas.nl/cpu: 'true'
        volumes:
          - name: ${CLUSTER_NAME}-nginx-config-vol
            configMap:
              name: ${CLUSTER_NAME}-nginx-config-spark
              items:
              - key: default.conf
                path: default.conf
          - name: ${CLUSTER_NAME}-secret-vol
            secret:
              secretName: ${CLUSTER_NAME}-secret
          - name: ${CLUSTER_NAME}-htpasswd-vol
            emptyDir: {}
          - name: ${CLUSTER_NAME}-vol
            persistentVolumeClaim:
              claimName: ${CLUSTER_NAME}-pvc
        initContainers:
        - image: apurva3000/alpine-htpasswd
          command: ["/bin/sh","-c","source /tmp/secret-env/secret.env && htpasswd -bc /tmp/secret-file/passwd $USER $PASS"]
          name: htpasswd-generator
          volumeMounts:
            - name: ${CLUSTER_NAME}-htpasswd-vol
              mountPath: "/tmp/secret-file"
            - name: ${CLUSTER_NAME}-secret-vol
              mountPath: "/tmp/secret-env"
        - name: busybox
          image: busybox:1
          args:
            - "/bin/mkdir"
            - "-p"
            - "/mnt/${CLUSTER_NAME}-pvc/.tools/${CLUSTER_NAME}/master"
          volumeMounts:
            - name: ${CLUSTER_NAME}-vol
              mountPath: /mnt/${CLUSTER_NAME}-pvc
        containers:
          - name: proxy-rewriter
            image: docker-registry.rahti.csc.fi/spark-images/nginx:latest
            imagePullPolicy: Always
            resources:
              requests:
                cpu: 100m
                memory: 128Mi
              limits:
                cpu: 300m
                memory: 512Mi
            ports:
              - containerPort: 9001
            volumeMounts:
              - name: ${CLUSTER_NAME}-nginx-config-vol
                mountPath: /opt/bitnami/nginx/conf/server_blocks
              - name: ${CLUSTER_NAME}-htpasswd-vol
                mountPath: /etc/nginx/secrets
          - name: spark-master
            image: ${MASTER_IMAGE}
            imagePullPolicy: Always
            ports:
              - containerPort: 7077
              - containerPort: 8080
            livenessProbe:
              httpGet:
                path: /
                port: 8080
              initialDelaySeconds: 30
              timeoutSeconds: 30
            resources:
              requests:
                  cpu: ${MASTER_CPU}
                  memory: ${MASTER_MEMORY}
              limits:
                  cpu: ${MASTER_CPU}
                  memory: ${MASTER_MEMORY}
            env:
              - name: START_COMMAND
                value: "/usr/local/bin/start-master.sh"
              - name: SPARK_PUBLIC_DNS
                value: ${CLUSTER_NAME}-spark.${APPLICATION_DOMAIN_SUFFIX}
              - name: SPARK_RECOVERY_DIR
                value: /mnt/${CLUSTER_NAME}-pvc/.tools/${CLUSTER_NAME}/master
            volumeMounts:
            - name: ${CLUSTER_NAME}-vol
              mountPath: /mnt/${CLUSTER_NAME}-pvc

- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    name: ${CLUSTER_NAME}-spark-worker
    labels:
      app: "${CLUSTER_NAME}"
  spec:
    replicas: ${WORKER_REPLICAS}
    template:
      metadata:
        annotations:
          description: Spark Worker
          template.alpha.openshift.io/wait-for-ready: "true"
        labels:
          app: ${CLUSTER_NAME}-spark-worker
      spec:
        nodeSelector:
          dsri.unimaas.nl/cpu: 'true'
        volumes:
          - name: ${CLUSTER_NAME}-vol
            persistentVolumeClaim:
              claimName: ${CLUSTER_NAME}-pvc
        containers:
          - name: spark-worker
            image: ${WORKER_IMAGE}
            imagePullPolicy: Always
            env:
              - name: SPARK_WORKER_MEMORY
                value: "${WORKER_MEMORY}"
              - name: SPARK_WORKER_CORES
                value: "${WORKER_CPU}"
              - name: SPARK_MASTER_SERVICE_HOST
                value: ${CLUSTER_NAME}-spark-master
              - name: SPARK_WORKER_DIRS
                value: "/tmp/worker"
              - name: START_COMMAND
                value: "/usr/local/bin/start-worker.sh"
            ports:
              - containerPort: 8081
            livenessProbe:
              httpGet:
                path: /
                port: 8081
              initialDelaySeconds: 30
              timeoutSeconds: 30
            resources:
              requests:
                  cpu: ${WORKER_CPU}
                  memory: ${WORKER_MEMORY}
              limits:
                  cpu: ${WORKER_CPU}
                  memory: ${WORKER_MEMORY}
            volumeMounts:
            - name: ${CLUSTER_NAME}-vol
              mountPath: /mnt/${CLUSTER_NAME}-pvc

- kind: ConfigMap
  apiVersion: v1
  metadata:
    name: "${CLUSTER_NAME}-cfg"
    labels:
      app: "${CLUSTER_NAME}"
  data:
    jupyter_notebook_config.py: |
      import os

      password = os.environ.get('JUPYTER_NOTEBOOK_PASSWORD')

      if password:
          import notebook.auth
          c.NotebookApp.password = notebook.auth.passwd(password)
          del password
          del os.environ['JUPYTER_NOTEBOOK_PASSWORD']

      image_config_file = '/home/jovyan/.jupyter/jupyter_notebook_config.py'

      if os.path.exists(image_config_file):
          with open(image_config_file) as fp:
              exec(compile(fp.read(), image_config_file, 'exec'), globals())

- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    annotations:
      description: Spark jupyter notebook
      template.alpha.openshift.io/wait-for-ready: "true"
    name: ${CLUSTER_NAME}-jupyter-notebook
    labels:
      app: "${CLUSTER_NAME}"
  spec:
    strategy:
      type: Rolling
    triggers:
      - type: ConfigChange
    replicas: 1
    template:
      metadata:
        labels:
          app: ${CLUSTER_NAME}-jupyter-notebook
      spec:
        # serviceAccountName: anyuid
        nodeSelector:
          dsri.unimaas.nl/cpu: 'true'
        automountServiceAccountToken: false
        securityContext:
          supplementalGroups:
          - 100
        volumes:
          - name: ${CLUSTER_NAME}-vol
            persistentVolumeClaim:
              claimName: ${CLUSTER_NAME}-pvc
          - configMap:
              name: "${CLUSTER_NAME}-cfg"
            name: configs
        #   - name: ${CLUSTER_NAME}-nginx-config-vol
        #     configMap:
        #       name: ${CLUSTER_NAME}-nginx-config-jupyter
        #   - name: ${CLUSTER_NAME}-secret-vol
        #     secret:
        #       secretName: ${CLUSTER_NAME}-secret
        #   - name: ${CLUSTER_NAME}-htpasswd-vol
        #     emptyDir: {}
        #   - name: ${CLUSTER_NAME}-jupyter-notebook-config-vol
        #     configMap:
        #       name: ${CLUSTER_NAME}-jupyter-notebook-config
        # initContainers:
        # - image: apurva3000/alpine-htpasswd
        #   command: ["/bin/sh","-c","source /tmp/secret-env/secret.env && htpasswd -bc /tmp/secret-file/passwd $USER $PASS"]
        #   name: htpasswd-generator
        #   volumeMounts:
        #     - name: ${CLUSTER_NAME}-htpasswd-vol
        #       mountPath: "/tmp/secret-file"
        #     - name: ${CLUSTER_NAME}-secret-vol
        #       mountPath: "/tmp/secret-env"
        containers:
          # - name: proxy-rewriter
          #   image: docker-registry.rahti.csc.fi/spark-images/nginx:latest
          #   imagePullPolicy: Always
          #   resources:
          #     requests:
          #       cpu: 100m
          #       memory: 128Mi
          #     limits:
          #       cpu: 300m
          #       memory: 512Mi
          #   ports:
          #     - containerPort: 8080
          #   volumeMounts:
          #     - name: ${CLUSTER_NAME}-nginx-config-vol
          #       mountPath: /opt/bitnami/nginx/conf/server_blocks
          #     - name: ${CLUSTER_NAME}-htpasswd-vol
          #       mountPath: /etc/nginx/secrets
          - name: jupyter
            image: ${JUPYTER_IMAGE}
            command:
            - start-notebook.sh
            - "--config=/etc/jupyter/openshift/jupyter_notebook_config.py"
            - "--no-browser"
            - "--ip=0.0.0.0"
            ports:
            - containerPort: 8888
              protocol: TCP
            env:
            # - name: START_COMMAND
            #   value: "/usr/local/bin/start-notebook.sh --NotebookApp.password='${NOTEBOOK_PASSWORD}'"
            - name: JUPYTER_NOTEBOOK_PASSWORD
              value: "${PASSWORD}"
            - name: JUPYTER_ENABLE_LAB
              value: ${JUPYTER_ENABLE_LAB} 
            # - name: GRANT_SUDO
            #   value: "yes"
            - name: SPARK_MASTER_SERVICE
              value: spark://${CLUSTER_NAME}-spark-master:7077
            - name: SPARK_OPTS
              value: --master=spark://${CLUSTER_NAME}-spark-master:7077
            - name: SPARK_DRIVER_MEMORY   # Need to set these values according to allocated resources
              value: ${DRIVER_MEMORY}
            - name: SPARK_DRIVER_CORES
              value: ${DRIVER_CPU}
            - name: SPARK_EXECUTOR_CORES_DEFAULT
              value: ${EXECUTOR_CORES_DEFAULT}
            - name: SPARK_EXECUTOR_MEMORY_DEFAULT
              value: ${EXECUTOR_MEMORY_DEFAULT}
            resources:
              requests:
                cpu: ${DRIVER_CPU}
                memory: ${DRIVER_MEMORY}
              limits:
                cpu: ${DRIVER_CPU}
                memory: ${DRIVER_MEMORY}
            # ports:
            # - containerPort: 8080
            volumeMounts:
            - mountPath: "/etc/jupyter/openshift"
              name: configs
            # - name: ${CLUSTER_NAME}-jupyter-notebook-config-vol
            #   mountPath: /home/jovyan/.jupyter/notebook_config.py
            #   subPath: notebook_config.py
            - name: ${CLUSTER_NAME}-vol
              mountPath: "/home/jovyan"


parameters:
- description: Unique identifier for your cluster. Recommended value - your username
  displayName: Cluster Name
  name: CLUSTER_NAME
  required: true
  value: spark-cluster

- description: Create a new username for logging into Spark UI and Jupyter
  displayName: Username
  name: USERNAME
  required: true
  value: spark

- description: Create a new password for logging into Spark UI
  displayName: Password
  name: PASSWORD
  required: true

- description: Number of Workers
  displayName: Worker Replicas
  name: WORKER_REPLICAS
  required: true
  value: '4'

- description: Persistent Storage Size
  displayName: Storage Size
  name: STORAGE_SIZE
  required: true
  value: '10Gi'

- description: Launch Jupyter Lab instead of Jupyter Notebook
  displayName: Enable Jupyter Lab
  name: JUPYTER_ENABLE_LAB
  required: true
  value: 'True'

- description: CPU for Master
  displayName: Master CPU
  name: MASTER_CPU
  required: true
  value: '1'

- description: Memory for Master
  displayName: Master Memory
  name: MASTER_MEMORY
  required: true
  value: 4G

- description: CPU for Worker
  displayName: Worker CPU
  name: WORKER_CPU
  required: true
  value: '2'

- description: Memory for Worker
  displayName: Worker Memory
  name: WORKER_MEMORY
  required: true
  value: 4G

- description: Default value for Spark Executor Cores
  displayName: Executor Default Cores
  name: EXECUTOR_CORES_DEFAULT
  required: true
  value: '2'

- description: Default value for Spark Executor Memory (Should always be less than the Worker memory!)
  displayName: Executor Default Memory
  name: EXECUTOR_MEMORY_DEFAULT
  required: true
  value: 3G

- description: CPU for Driver(Jupyter)
  displayName: Driver CPU
  name: DRIVER_CPU
  required: true
  value: '1'

- description: Memory for Driver(Jupyter)
  displayName: Driver Memory
  name: DRIVER_MEMORY
  required: true
  value: 4G

- description: Docker Image for the Master
  displayName: Master Image
  name: MASTER_IMAGE
  required: true
  value: docker-registry.rahti.csc.fi/spark-images/spark:2.4

- description: Docker Image for the Worker
  displayName: Worker Image
  name: WORKER_IMAGE
  required: true
  value: docker-registry.rahti.csc.fi/spark-images/spark:2.4

- description: Docker Image for the Jupyter (Driver)
  displayName: Worker Image
  name: JUPYTER_IMAGE
  required: true
  value: jupyter/all-spark-notebook:latest
  # value: umids/jupyterlab-spark:latest
  # value: docker-registry.rahti.csc.fi/spark-images/spark:2.4

- description: The exposed hostname suffix that will be used to create routes for Spark UI and Jupyter Notebook
  displayName: Application Hostname Suffix
  name: APPLICATION_DOMAIN_SUFFIX
  value: app.dsri.unimaas.nl
